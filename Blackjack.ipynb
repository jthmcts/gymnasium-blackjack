{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install huggingface_hub\n",
        "!pip install pygame\n",
        "!pip install numpy\n",
        "\n",
        "!pip install dill\n",
        "!pip install pyyaml==6.0\n",
        "!pip install imageio\n",
        "!pip install imageio_ffmpeg\n",
        "!pip install pyglet==1.5.1\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "w0o-D8atsm32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "P3_Jit4z3CsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "1OEv6fP63WO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(400, 300))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "9F3xg1R63Jwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTiHb9L7rPzp"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "import dill as pickle\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Blackjack-v1\", sab=True, render_mode=\"rgb_array\")"
      ],
      "metadata": {
        "id": "iTOaW2fZPuEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_q_values(env):\n",
        "  return defaultdict(lambda: np.zeros(env.action_space.n))"
      ],
      "metadata": {
        "id": "PSDDAFWagOqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_values = initialize_q_values(env)"
      ],
      "metadata": {
        "id": "dqAu8GIOQegV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_policy(q_values, state):\n",
        "  return int(np.argmax(q_values[state]))"
      ],
      "metadata": {
        "id": "LgcKPRk0Qq6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epilson_greedy_policy(q_values, state, epsilon):\n",
        "  random_num = np.random.random()\n",
        "  if random_num < epsilon:\n",
        "    return env.action_space.sample()\n",
        "  # with probability (1 - epsilon) act greedily (exploit)\n",
        "  else:\n",
        "    return greedy_policy(q_values, state)"
      ],
      "metadata": {
        "id": "oGO9dcq2Q_sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "\n",
        "# Training parameters\n",
        "n_training_episodes = 500_000\n",
        "learning_rate = 0.001\n",
        "discount_factor = 0.95\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.05\n",
        "decay_rate = 0.0001"
      ],
      "metadata": {
        "id": "I-lW_CRgSNFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsion, decay_rate, q_values):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    epsilon = max_epsilon\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # play one episode\n",
        "    while not done:\n",
        "        action = epilson_greedy_policy(q_values, state, epsilon)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        future_q_value = (not terminated) * np.max(q_values[next_state])\n",
        "        temporal_difference = (\n",
        "            reward + discount_factor * future_q_value - q_values[state][action]\n",
        "        )\n",
        "\n",
        "        q_values[state][action] = (\n",
        "            q_values[state][action] + learning_rate * temporal_difference\n",
        "        )\n",
        "\n",
        "        # update if the environment is done and the current obs\n",
        "        done = terminated or truncated\n",
        "        state = next_state\n",
        "\n",
        "    epsilon = max(min_epsilon, epsilon - decay_rate)\n",
        "  return q_values"
      ],
      "metadata": {
        "id": "QDizq4gLTDqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_values = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, q_values)"
      ],
      "metadata": {
        "id": "BZSJ3I6rVsxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, n_eval_episodes, q_values):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param Q: The q values\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    truncated = False\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    done = False\n",
        "    # play one episode\n",
        "    while not done:\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = greedy_policy(q_values, state)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "8r52wibyOoSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "mean_reward, std_reward = evaluate_agent(env, n_eval_episodes, q_values)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "ThdU5N8vPC7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def record_video(env, q_values, out_directory, fps=1):\n",
        "    \"\"\"\n",
        "    Generate a replay video of the agent\n",
        "    :param env\n",
        "    :param q_values: q_values of our agent\n",
        "    :param out_directory\n",
        "    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    state, info = env.reset(seed=random.randint(0, 500))\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "    for step in range(20):\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = greedy_policy(q_values, state)\n",
        "        state, reward, terminated, truncated, info = env.step(action)  # We directly put next_state = state for recording logic\n",
        "        img = env.render()\n",
        "        images.append(img)\n",
        "        if terminated or truncated:\n",
        "          state, info = env.reset(seed=random.randint(0, 500))\n",
        "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ],
      "metadata": {
        "id": "nhi6pqIN8pq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, snapshot_download\n",
        "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import json"
      ],
      "metadata": {
        "id": "q9-o3iwyeA--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def push_to_hub(\n",
        "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
        "    This method does the complete pipeline:\n",
        "    - It evaluates the model\n",
        "    - It generates the model card\n",
        "    - It generates a replay video of the agent\n",
        "    - It pushes everything to the Hub\n",
        "\n",
        "    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
        "    :param env\n",
        "    :param video_fps: how many frame per seconds to record our video replay\n",
        "    (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "    :param local_repo_path: where the local repository is\n",
        "    \"\"\"\n",
        "    _, repo_name = repo_id.split(\"/\")\n",
        "\n",
        "    eval_env = env\n",
        "    api = HfApi()\n",
        "\n",
        "    # Step 1: Create the repo\n",
        "    repo_url = api.create_repo(\n",
        "        repo_id=repo_id,\n",
        "        exist_ok=True,\n",
        "    )\n",
        "\n",
        "    # Step 2: Download files\n",
        "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
        "\n",
        "    # Step 3: Save the model\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
        "\n",
        "    # Pickle the model\n",
        "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Step 4: Evaluate the model and build JSON with evaluation metrics\n",
        "    mean_reward, std_reward = evaluate_agent(\n",
        "        eval_env, model[\"n_eval_episodes\"], q_values\n",
        "    )\n",
        "\n",
        "    evaluate_data = {\n",
        "        \"env_id\": model[\"env_id\"],\n",
        "        \"mean_reward\": mean_reward,\n",
        "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
        "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Write a JSON file called \"results.json\" that will contain the\n",
        "    # evaluation results\n",
        "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
        "        json.dump(evaluate_data, outfile)\n",
        "\n",
        "    # Step 5: Create the model card\n",
        "    env_name = model[\"env_id\"]\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
        "\n",
        "    metadata = {}\n",
        "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
        "\n",
        "    # Add metrics\n",
        "    eval = metadata_eval_result(\n",
        "        model_pretty_name=repo_name,\n",
        "        task_pretty_name=\"reinforcement-learning\",\n",
        "        task_id=\"reinforcement-learning\",\n",
        "        metrics_pretty_name=\"mean_reward\",\n",
        "        metrics_id=\"mean_reward\",\n",
        "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
        "        dataset_pretty_name=env_name,\n",
        "        dataset_id=env_name,\n",
        "    )\n",
        "    env_id = model[\"env_id\"]\n",
        "\n",
        "    # Merges both dictionaries\n",
        "    metadata = {**metadata, **eval}\n",
        "\n",
        "    model_card = f\"\"\"\n",
        "  # **Q-Learning** Agent playing1 **{env_id}**\n",
        "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
        "\n",
        "  ## Usage\n",
        "\n",
        "  ```python\n",
        "\n",
        "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
        "\n",
        "  # Don't forget to check if you need to add additional attributes\n",
        "  env = gym.make(model[\"env_id\"])\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "    evaluate_agent(env, model[\"n_eval_episodes\"], model[\"q_values\"])\n",
        "\n",
        "    readme_path = repo_local_path / \"README.md\"\n",
        "    readme = \"\"\n",
        "    print(readme_path.exists())\n",
        "    if readme_path.exists():\n",
        "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "            readme = f.read()\n",
        "    else:\n",
        "        readme = model_card\n",
        "\n",
        "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    # Save our metrics to Readme metadata\n",
        "    metadata_save(readme_path, metadata)\n",
        "\n",
        "    # Step 6: Record a video\n",
        "    video_path = repo_local_path / \"replay.mp4\"\n",
        "    record_video(env, model[\"q_values\"], video_path, video_fps)\n",
        "\n",
        "    # Step 7. Push everything to the Hub\n",
        "    api.upload_folder(\n",
        "        repo_id=repo_id,\n",
        "        folder_path=repo_local_path,\n",
        "        path_in_repo=\".\",\n",
        "    )\n",
        "\n",
        "    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)"
      ],
      "metadata": {
        "id": "KIa51efo_7F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "i29xkRtOcvac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = {\n",
        "    \"env_id\": \"BlackJack-v1\",\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"discount_factor\": discount_factor,\n",
        "    \"q_values\": q_values\n",
        "}"
      ],
      "metadata": {
        "id": "bJ8wSelEc1Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "username = \"nzdb70\" # FILL THIS\n",
        "repo_name = \"BlackJack-v1\"\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "t5Ba5kAEdnbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}